{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLTN+QWmY3xK5q2ThyRPS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smiliyo55/psoCNN_Brain_Tumor/blob/main/psoCNN1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHkVBiVKk34F"
      },
      "outputs": [],
      "source": [
        "!pip install import_ipynb\n",
        "!pip install torchsummary\n",
        "import import_ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from copy import deepcopy\n",
        "\n",
        "import utils1\n",
        "import particle1\n",
        "from population1 import Population\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "bsvQA0ValGAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class psoCNN:\n",
        "  def __init__(self, dataset, n_iter, pop_size, batch_size, epochs, min_layer, max_layer, \\\n",
        "      conv_prob, pool_prob, fc_prob, max_conv_kernel, max_out_ch, max_fc_neurons, dropout_rate):\n",
        "\n",
        "    self.pop_size = pop_size\n",
        "    self.n_iter = n_iter\n",
        "    self.epochs = epochs\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.gBest_acc = np.zeros(n_iter)\n",
        "    self.gBest_test_acc = np.zeros(n_iter)\n",
        "\n",
        "    self.ctr = 0\n",
        "    self.ctr1 = 0\n",
        "\n",
        "    dataset = \"Brain Tumor MRI Dataset\"\n",
        "    input_width = 224\n",
        "    input_height = 224\n",
        "    input_channels = 3\n",
        "    output_dim = 4\n",
        "\n",
        "    # Define transformations\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    trainset = ImageFolder('/content/drive/MyDrive/Colab Notebooks/psoCNN2/Training', transform=train_transform)\n",
        "\n",
        "    testset = ImageFolder('/content/drive/MyDrive/Colab Notebooks/psoCNN2/Testing', transform=test_transform)\n",
        "\n",
        "    # Create data loaders\n",
        "    self.trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size, shuffle=True)\n",
        "    self.testloader = torch.utils.data.DataLoader(testset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    self.results_path = \"/content/drive/MyDrive/Colab Notebooks/psoCNN2/results/\" + dataset + \"/\"\n",
        "\n",
        "    self.output_str = \"Initializing population...\"  + \"\\n\"\n",
        "\n",
        "    print(\"Initializing population...\")\n",
        "    self.population = Population(pop_size, min_layer, max_layer, input_width, input_height, input_channels,\n",
        "                                  conv_prob, pool_prob, fc_prob, max_conv_kernel, max_out_ch, max_fc_neurons, output_dim)\n",
        "\n",
        "    print(\"Verifying accuracy of the current gBest...\"+ \"\\n\")\n",
        "    self.output_str = self.output_str + \"Particle - \" + str(1) + \" architecture : \" + str(self.population.particle[0]) + \"\\n\"\n",
        "    self.output_str = self.output_str + json.dumps(self.population.particle[0].layers) + \"\\n\"\n",
        "\n",
        "    #self.population.particle[0].layers = [{'type': 'conv', 'ou_c': 16, 'kernel': 3}, {'type': 'max_pool', 'ou_c': 16, 'kernel': 2}, {'type': 'conv', 'ou_c': 32, 'kernel': 3}, {'type': 'max_pool', 'ou_c': 32, 'kernel': 2}, {'type': 'fc', 'ou_c': 128, 'kernel': -1}, {'type': 'fc', 'ou_c': 4, 'kernel': -1}]\n",
        "    self.population.particle[0].model_compile(dropout_rate)\n",
        "    print(self.population.particle[0])\n",
        "    print(self.population.particle[0].layers)\n",
        "    print()\n",
        "\n",
        "    hist = self.population.particle[0].model_fit(self.trainloader, self.testloader, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "    self.gBest = deepcopy(self.population.particle[0])\n",
        "    self.population.particle[0].pBest = deepcopy(self.population.particle[0])\n",
        "    self.population.particle[0].pBest.model_delete()\n",
        "\n",
        "    self.gBest_acc[0] = hist['train accuracy'][-1]\n",
        "    self.gBest_test_acc[0] = hist['val accuracy'][-1]\n",
        "\n",
        "    self.population.particle[0].acc = hist['train accuracy'][-1]\n",
        "    self.population.particle[0].pBest.acc = hist['train accuracy'][-1]\n",
        "\n",
        "    self.population.particle[0].model_delete()\n",
        "\n",
        "    print(\"Current gBest acc: \" + str(self.gBest_acc[0]) + \"\\n\")\n",
        "    print(\"Current gBest test acc: \" + str(self.gBest_test_acc[0]) + \"\\n\")\n",
        "\n",
        "    print(\"Looking for a new gBest in the population...\")\n",
        "    self.redd_count = 0\n",
        "    for i in range(1, self.pop_size):\n",
        "        print('Initialization - Particle: ' + str(i+1))\n",
        "        self.output_str = self.output_str + \"Particle - \" + str(i+1) + \" architecture : \" + str(self.population.particle[i]) + \"\\n\"\n",
        "        self.output_str = self.output_str + json.dumps(self.population.particle[i].layers) + \"\\n\"\n",
        "        print(self.population.particle[i])\n",
        "        print(self.population.particle[i].layers)\n",
        "\n",
        "        skip_iteration = False\n",
        "        for j in range(i):\n",
        "            if self.population.particle[j] == self.population.particle[i] :\n",
        "                skip_iteration = True\n",
        "                self.redd_count += 1\n",
        "                print('Particule ' + str(i+1) + ' redondante')\n",
        "                self.population.particle[i].acc = self.population.particle[j].acc\n",
        "                self.population.particle[i].pBest.acc = self.population.particle[i].acc\n",
        "                break\n",
        "\n",
        "        if skip_iteration:\n",
        "            continue\n",
        "\n",
        "        self.population.particle[i].model_compile(dropout_rate)\n",
        "        hist = self.population.particle[i].model_fit(self.trainloader, self.testloader, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "        self.population.particle[i].acc = hist['train accuracy'][-1]\n",
        "        self.population.particle[i].pBest.acc = hist['train accuracy'][-1]\n",
        "\n",
        "        if self.population.particle[i].pBest.acc > self.gBest_acc[0]:\n",
        "            print(\"Found a new gBest.\")\n",
        "            self.gBest.model_delete()\n",
        "            self.gBest = deepcopy(self.population.particle[i])\n",
        "            self.gBest_acc[0] = self.population.particle[i].pBest.acc\n",
        "            print(\"New gBest acc: \" + str(self.gBest_acc[0]))\n",
        "            self.gBest_test_acc[0] = hist['val accuracy'][-1]\n",
        "            print(\"New gBest test acc: \" + str(self.gBest_test_acc[0]))\n",
        "\n",
        "        self.population.particle[i].model_delete()\n",
        "\n",
        "  def fit(self, Cg, dropout_rate):\n",
        "    for i in range(1, self.n_iter):\n",
        "      gBest_acc = self.gBest_acc[i-1]\n",
        "      gBest_test_acc = self.gBest_test_acc[i-1]\n",
        "\n",
        "      self.ctr += 1\n",
        "\n",
        "      self.output_str = self.output_str + 'Iteration: ' + str(i) + \"\\n\"\n",
        "      incremented = False\n",
        "      for j in range(self.pop_size):\n",
        "        print('Iteration: ' + str(i) + ' - Particle: ' + str(j+1))\n",
        "\n",
        "        # Update particle velocity\n",
        "        self.population.particle[j].velocity(self.gBest.layers, Cg)\n",
        "\n",
        "        # Update particle architecture\n",
        "        self.population.particle[j].update()\n",
        "\n",
        "        print('Particle NEW architecture: ')\n",
        "        self.output_str = self.output_str + 'Iteration: ' + str(i) + \"Particle - \" + str(j+1) + \" architecture : \" + str(self.population.particle[j]) + \"\\n\"\n",
        "        self.output_str = self.output_str + json.dumps(self.population.particle[j].layers) + \"\\n\"\n",
        "\n",
        "        print(self.population.particle[j])\n",
        "        print(self.population.particle[j].layers)\n",
        "\n",
        "        self.population.particle[j].model_compile(dropout_rate)\n",
        "\n",
        "        skip_iteration = False\n",
        "        for k in range(j):\n",
        "            if self.population.particle[k] == self.population.particle[j] :\n",
        "                skip_iteration = True\n",
        "                self.redd_count += 1\n",
        "                print('Particule ' + str(j+1) + ' redondante')\n",
        "\n",
        "                self.population.particle[j].acc = self.population.particle[k].acc\n",
        "\n",
        "                f_test = self.population.particle[j].acc\n",
        "                pBest_acc = self.population.particle[j].pBest.acc\n",
        "\n",
        "                if f_test > pBest_acc:\n",
        "                    print(\"Found a new pBest.\")\n",
        "                    print(\"Current acc: \" + str(f_test))\n",
        "                    print(\"Past pBest acc: \" + str(pBest_acc))\n",
        "                    self.population.particle[j].pBest = deepcopy(self.population.particle[k])\n",
        "                break\n",
        "\n",
        "        if skip_iteration:\n",
        "            continue\n",
        "\n",
        "        for k in range(j+1, self.pop_size):\n",
        "            if self.population.particle[k] == self.population.particle[j] :\n",
        "                skip_iteration = True\n",
        "                self.redd_count += 1\n",
        "                print('Particule ' + str(j+1) + ' redondante')\n",
        "\n",
        "                self.population.particle[j].acc = self.population.particle[k].acc\n",
        "\n",
        "                f_test = self.population.particle[j].acc\n",
        "                pBest_acc = self.population.particle[j].pBest.acc\n",
        "\n",
        "                if f_test > pBest_acc:\n",
        "                    print(\"Found a new pBest.\")\n",
        "                    print(\"Current acc: \" + str(f_test))\n",
        "                    print(\"Past pBest acc: \" + str(pBest_acc))\n",
        "                    self.population.particle[j].pBest = deepcopy(self.population.particle[k])\n",
        "\n",
        "                break\n",
        "\n",
        "        if skip_iteration:\n",
        "            continue\n",
        "\n",
        "        # Compute the acc in the updated particle\n",
        "\n",
        "        hist = self.population.particle[j].model_fit(self.trainloader, self.testloader, batch_size=self.batch_size, epochs=self.epochs + self.ctr)\n",
        "\n",
        "        self.population.particle[j].acc = hist['train accuracy'][-1]\n",
        "\n",
        "        f_test = self.population.particle[j].acc\n",
        "        pBest_acc = self.population.particle[j].pBest.acc\n",
        "\n",
        "        if f_test > pBest_acc:\n",
        "            print(\"Found a new pBest.\")\n",
        "            print(\"Current acc: \" + str(f_test))\n",
        "            print(\"Past pBest acc: \" + str(pBest_acc))\n",
        "            pBest_acc = f_test\n",
        "            self.population.particle[j].pBest = deepcopy(self.population.particle[j])\n",
        "            self.population.particle[j].pBest.model_delete()\n",
        "\n",
        "            if pBest_acc > gBest_acc:\n",
        "                print(\"Found a new gBest.\")\n",
        "                self.gBest.model_delete()\n",
        "                gBest_acc = pBest_acc\n",
        "                self.gBest = deepcopy(self.population.particle[j])\n",
        "                gBest_test_acc = hist['val accuracy'][-1]\n",
        "                if not incremented:\n",
        "                  self.ctr1 += 1\n",
        "\n",
        "        self.population.particle[j].model_delete()\n",
        "\n",
        "\n",
        "      self.gBest_acc[i] = gBest_acc\n",
        "      self.gBest_test_acc[i] = gBest_test_acc\n",
        "\n",
        "      print(\"Current gBest acc: \" + str(self.gBest_acc[i]))\n",
        "      print(\"Current gBest test acc: \" + str(self.gBest_test_acc[i]))\n",
        "\n",
        "  def fit_gBest(self, batch_size, epochs1, epochs, dropout_rate):\n",
        "    print(\"\\nFurther training gBest model...\")\n",
        "    #self.gBest.model_compile(dropout_rate)\n",
        "    self.output_str = self.output_str + \"gBest-architecture : \" + str(self.gBest) + \"\\n\"\n",
        "    self.output_str = self.output_str + json.dumps(self.gBest.layers) + \"\\n\"\n",
        "\n",
        "    with open(self.results_path + \"swarms.txt\", \"w\") as f:\n",
        "      try:\n",
        "          print(self.output_str, file=f)\n",
        "      except SyntaxError:\n",
        "          print >> f, self.output_str\n",
        "\n",
        "    print(self.gBest)\n",
        "    print(self.gBest.layers)\n",
        "    self.gBest.model_summary()\n",
        "\n",
        "    trainable_count = count_parameters(self.gBest.model)\n",
        "    print(\"gBest's number of trainable parameters: \" + str(trainable_count))\n",
        "\n",
        "    metrics = self.gBest.model_fit_complete(self.trainloader, self.testloader, batch_size=batch_size, epochs1=epochs1 + self.ctr1, epochs=epochs)\n",
        "\n",
        "    print(\"\\ngBest model loss in the test set: \" + str(metrics['val loss'][-1]) + \" - Test set accuracy: \" + str(metrics['val accuracy'][-1]))\n",
        "\n",
        "    self.gBest.model_delete()\n",
        "\n",
        "    return trainable_count, metrics"
      ],
      "metadata": {
        "id": "zqyHhb-alMH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}