{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smiliyo55/psoCNN_Brain_Tumor/blob/main/psoCNN1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB8E9KoxhaKp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import_ipynb\n",
        "!pip install torchsummary\n",
        "import import_ipynb"
      ],
      "metadata": {
        "id": "tM37kHgKiE-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import utils1\n",
        "import particle1\n",
        "from population1 import Population"
      ],
      "metadata": {
        "id": "B89xnhuIYph7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "8u_s6ulO8sNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class psoCNN:\n",
        "  def __init__(self, dataset, n_iter, pop_size, batch_size, epochs, min_layer, max_layer, \\\n",
        "      conv_prob, pool_prob, fc_prob, max_conv_kernel, max_out_ch, max_fc_neurons, dropout_rate):\n",
        "\n",
        "    self.pop_size = pop_size\n",
        "    self.n_iter = n_iter\n",
        "    self.epochs = epochs\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.gBest_acc = np.zeros(n_iter)\n",
        "    self.gBest_test_acc = np.zeros(n_iter)\n",
        "\n",
        "    dataset = \"Brain Tumor MRI Dataset\"\n",
        "    input_width = 224\n",
        "    input_height = 224\n",
        "    input_channels = 3\n",
        "    output_dim = 4\n",
        "\n",
        "    # Define transformations\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    trainset = ImageFolder('/content/drive/MyDrive/Colab Notebooks/psoCNN2/Training', transform=transform)\n",
        "\n",
        "    testset = ImageFolder('/content/drive/MyDrive/Colab Notebooks/psoCNN2/Testing', transform=transform)\n",
        "\n",
        "\n",
        "\n",
        "    # Create data loaders\n",
        "    self.trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size,\n",
        "                                              shuffle=True)\n",
        "    self.testloader = torch.utils.data.DataLoader(testset, batch_size=self.batch_size,\n",
        "                                            shuffle=False)\n",
        "\n",
        "    print(\"Initializing population...\")\n",
        "    self.population = Population(pop_size, min_layer, max_layer, input_width, input_height, input_channels,\n",
        "                                  conv_prob, pool_prob, fc_prob, max_conv_kernel, max_out_ch, max_fc_neurons, output_dim)\n",
        "\n",
        "    print(\"Verifying accuracy of the current gBest...\")\n",
        "    print(self.population.particle[0])\n",
        "    self.gBest = deepcopy(self.population.particle[0])\n",
        "    self.gBest.model_compile(dropout_rate)\n",
        "    hist = self.gBest.model_fit(self.trainloader, batch_size=batch_size, epochs=epochs)\n",
        "    test_metrics = self.gBest.evaluate(self.testloader, batch_size)\n",
        "    self.gBest.model_delete()\n",
        "\n",
        "    self.gBest_acc[0] = hist['accuracy'][-1]\n",
        "    self.gBest_test_acc[0] = test_metrics['accuracy']\n",
        "\n",
        "    self.population.particle[0].acc = hist['accuracy'][-1]\n",
        "    self.population.particle[0].pBest.acc = hist['accuracy'][-1]\n",
        "\n",
        "    print(\"Current gBest acc: \" + str(self.gBest_acc[0]) + \"\\n\")\n",
        "    print(\"Current gBest test acc: \" + str(self.gBest_test_acc[0]) + \"\\n\")\n",
        "\n",
        "    print(\"Looking for a new gBest in the population...\")\n",
        "    for i in range(1, self.pop_size):\n",
        "        print('Initialization - Particle: ' + str(i+1))\n",
        "        print(self.population.particle[i])\n",
        "\n",
        "        self.population.particle[i].model_compile(dropout_rate)\n",
        "        hist = self.population.particle[i].model_fit(self.trainloader, batch_size=batch_size, epochs=epochs)\n",
        "        #self.population.particle[i].model_delete()\n",
        "\n",
        "        self.population.particle[i].acc = hist['accuracy'][-1]\n",
        "        self.population.particle[i].pBest.acc = hist['accuracy'][-1]\n",
        "\n",
        "        if self.population.particle[i].pBest.acc >= self.gBest_acc[0]:\n",
        "            print(\"Found a new gBest.\")\n",
        "            self.gBest = deepcopy(self.population.particle[i])\n",
        "            self.gBest_acc[0] = self.population.particle[i].pBest.acc\n",
        "            print(\"New gBest acc: \" + str(self.gBest_acc[0]))\n",
        "\n",
        "            #self.gBest.model_compile(dropout_rate)\n",
        "            test_metrics = self.gBest.evaluate(self.testloader, batch_size=batch_size)\n",
        "            self.gBest_test_acc[0] = test_metrics['accuracy']\n",
        "            print(\"New gBest test acc: \" + str(self.gBest_acc[0]))\n",
        "            self.gBest.model_delete()\n",
        "\n",
        "        self.population.particle[i].model_delete()\n",
        "\n",
        "  def fit(self, Cg, dropout_rate):\n",
        "    for i in range(1, self.n_iter):\n",
        "      gBest_acc = self.gBest_acc[i-1]\n",
        "      gBest_test_acc = self.gBest_test_acc[i-1]\n",
        "\n",
        "      for j in range(self.pop_size):\n",
        "        print('Iteration: ' + str(i) + ' - Particle: ' + str(j+1))\n",
        "\n",
        "        # Update particle velocity\n",
        "        self.population.particle[j].velocity(self.gBest.layers, Cg)\n",
        "\n",
        "        # Update particle architecture\n",
        "        self.population.particle[j].update()\n",
        "\n",
        "        print('Particle NEW architecture: ')\n",
        "        print(self.population.particle[j])\n",
        "\n",
        "        # Compute the acc in the updated particle\n",
        "        self.population.particle[j].model_compile(dropout_rate)\n",
        "        hist = self.population.particle[j].model_fit(self.trainloader, batch_size=self.batch_size, epochs=self.epochs)\n",
        "        #self.population.particle[j].model_delete()\n",
        "\n",
        "        self.population.particle[j].acc = hist['accuracy'][-1]\n",
        "\n",
        "        f_test = self.population.particle[j].acc\n",
        "        pBest_acc = self.population.particle[j].pBest.acc\n",
        "\n",
        "        if f_test >= pBest_acc:\n",
        "            print(\"Found a new pBest.\")\n",
        "            print(\"Current acc: \" + str(f_test))\n",
        "            print(\"Past pBest acc: \" + str(pBest_acc))\n",
        "            pBest_acc = f_test\n",
        "            self.population.particle[j].pBest = deepcopy(self.population.particle[j])\n",
        "            self.population.particle[j].pBest.model_delete()\n",
        "\n",
        "            if pBest_acc >= gBest_acc:\n",
        "                print(\"Found a new gBest.\")\n",
        "                gBest_acc = pBest_acc\n",
        "                self.gBest = deepcopy(self.population.particle[j])\n",
        "\n",
        "                #self.gBest.model_compile(dropout_rate)\n",
        "                #hist = self.gBest.model_fit(self.trainloader, batch_size=self.batch_size, epochs=self.epochs)\n",
        "                test_metrics = self.gBest.evaluate(self.testloader, batch_size=self.batch_size)\n",
        "                self.gBest.model_delete()\n",
        "                gBest_test_acc = test_metrics['accuracy']\n",
        "\n",
        "        self.population.particle[j].model_delete()\n",
        "\n",
        "      self.gBest_acc[i] = gBest_acc\n",
        "      self.gBest_test_acc[i] = gBest_test_acc\n",
        "\n",
        "      print(\"Current gBest acc: \" + str(self.gBest_acc[i]))\n",
        "      print(\"Current gBest test acc: \" + str(self.gBest_test_acc[i]))\n",
        "\n",
        "  def fit_gBest(self, batch_size, epochs, dropout_rate):\n",
        "    print(\"\\nFurther training gBest model...\")\n",
        "    self.gBest.model_compile(dropout_rate)\n",
        "    print(self.gBest)\n",
        "    self.gBest.model_summary()\n",
        "\n",
        "    trainable_count = particle1.count_parameters(self.gBest.model)\n",
        "    print(\"gBest's number of trainable parameters: \" + str(trainable_count))\n",
        "\n",
        "    metrics = self.gBest.model_fit_complete(self.trainloader, self.testloader, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "    print(\"\\ngBest model loss in the test set: \" + str(metrics['val loss'][-1]) + \" - Test set accuracy: \" + str(metrics['val accuracy'][-1]))\n",
        "\n",
        "    self.gBest.model_delete()\n",
        "\n",
        "    return trainable_count, metrics"
      ],
      "metadata": {
        "id": "7bF3ohdbx11P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}