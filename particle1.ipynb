{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smiliyo55/psoCNN_Brain_Tumor/blob/main/particle1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJpI1qOIX00_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import_ipynb\n",
        "import import_ipynb"
      ],
      "metadata": {
        "id": "hEl5juoeYBDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "import utils1\n",
        "\n",
        "import os\n",
        "from torchsummary import summary\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "T51w1Rn0Zpli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Model ####\n",
        "class p_model(nn.Sequential):\n",
        "  def __init__(self, list_layers, input_width, input_height, input_channels, output_dim, dropout_rate) :\n",
        "    super(p_model, self).__init__()\n",
        "\n",
        "    in_w = input_width\n",
        "    in_h = input_height\n",
        "\n",
        "    self.list_layers = list_layers\n",
        "\n",
        "    self.model = nn.Sequential()\n",
        "\n",
        "    for i in range(len(list_layers)):\n",
        "        if list_layers[i][\"type\"] == \"conv\":\n",
        "            n_out_filters = list_layers[i][\"ou_c\"]\n",
        "            kernel_size = list_layers[i][\"kernel\"]\n",
        "\n",
        "            if i == 0:\n",
        "                in_c = input_channels\n",
        "\n",
        "                p = int(( kernel_size - 1 ) / 2)\n",
        "                if kernel_size % 2 == 0:\n",
        "                  self.model.add_module(f'padding{i+1}', nn.ZeroPad2d((p, p+1, p, p+1)))\n",
        "                  self.model.add_module(f'conv{i+1}', nn.Conv2d(\n",
        "                                    in_channels=in_c,\n",
        "                                    out_channels=n_out_filters,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    stride=1,\n",
        "                                    bias=True\n",
        "                                    ))\n",
        "                  self.model.add_module(f'relu{i+1}', nn.ReLU(inplace=True))\n",
        "                  self.model.add_module(f'batchnorm{i+1}', nn.BatchNorm2d(num_features=n_out_filters))\n",
        "                else:\n",
        "                  self.model.add_module(f'padding{i+1}', nn.ZeroPad2d((p, p, p, p)))\n",
        "                  self.model.add_module(f'conv{i+1}', nn.Conv2d(\n",
        "                                    in_channels=in_c,\n",
        "                                    out_channels=n_out_filters,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    stride=1,\n",
        "                                    bias=True\n",
        "                                    ))\n",
        "                  self.model.add_module(f'relu{i+1}', nn.ReLU(inplace=True))\n",
        "                  self.model.add_module(f'batchnorm{i+1}', nn.BatchNorm2d(num_features=n_out_filters))\n",
        "            else:\n",
        "                in_c = list_layers[i-1][\"ou_c\"]\n",
        "\n",
        "                #self.model.add_module(f'batchnorm{i+1}', nn.Dropout(dropout_rate))\n",
        "                p = int(( kernel_size - 1 ) / 2)\n",
        "                if kernel_size % 2 == 0:\n",
        "                  self.model.add_module(f'padding{i+1}', nn.ZeroPad2d((p, p+1, p, p+1)))\n",
        "                  self.model.add_module(f'conv{i+1}', nn.Conv2d(\n",
        "                                    in_channels=in_c,\n",
        "                                    out_channels=n_out_filters,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    stride=1,\n",
        "                                    bias=True\n",
        "                                    ))\n",
        "                  self.model.add_module(f'relu{i+1}', nn.ReLU(inplace=True))\n",
        "                  self.model.add_module(f'batchnorm{i+1}', nn.BatchNorm2d(num_features=n_out_filters))\n",
        "                else:\n",
        "                  self.model.add_module(f'padding{i+1}', nn.ZeroPad2d((p, p, p, p)))\n",
        "                  self.model.add_module(f'conv{i+1}', nn.Conv2d(\n",
        "                                    in_channels=in_c,\n",
        "                                    out_channels=n_out_filters,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    stride=1,\n",
        "                                    bias=True\n",
        "                                    ))\n",
        "                  self.model.add_module(f'relu{i+1}', nn.ReLU(inplace=True))\n",
        "                  self.model.add_module(f'batchnorm{i+1}', nn.BatchNorm2d(num_features=n_out_filters))\n",
        "\n",
        "        if list_layers[i][\"type\"] == \"max_pool\":\n",
        "            kernel_size = list_layers[i][\"kernel\"]\n",
        "            self.list_layers[i][\"ou_c\"] = list_layers[i-1][\"ou_c\"]\n",
        "\n",
        "            self.model.add_module(f'maxpool{i+1}', nn.MaxPool2d(kernel_size=kernel_size, stride=2))\n",
        "\n",
        "            in_w = int(in_w/2)\n",
        "            in_h = int(in_h/2)\n",
        "            #print(in_w, \", \", in_h)\n",
        "\n",
        "        if list_layers[i][\"type\"] == \"avg_pool\":\n",
        "            kernel_size = list_layers[i][\"kernel\"]\n",
        "            self.list_layers[i][\"ou_c\"] = list_layers[i-1][\"ou_c\"]\n",
        "\n",
        "            self.model.add_module(f'avgpool{i+1}', nn.AvgPool2d(kernel_size=kernel_size, stride=2))\n",
        "\n",
        "            in_w = int(in_w/2)\n",
        "            in_h = int(in_h/2)\n",
        "            #print(in_w, \", \", in_h)\n",
        "\n",
        "        if list_layers[i][\"type\"] == \"fc\":\n",
        "          if list_layers[i-1][\"type\"] != \"fc\":\n",
        "            self.model.add_module('flatten', nn.Flatten())\n",
        "\n",
        "            if i == len(list_layers) - 1:\n",
        "              in_c = list_layers[i-1][\"ou_c\"]\n",
        "              in_f = in_c * in_w * in_h\n",
        "              #print(in_f)\n",
        "              self.model.add_module(f'linear{i+1}', nn.Linear(in_features=in_f, out_features=output_dim))\n",
        "              #self.model.add_module(f'batchnorm{i+1}', nn.BatchNorm1d(num_features=output_dim))\n",
        "              #self.model.add_module('softmax', nn.Softmax(dim=1))\n",
        "            else:\n",
        "              in_c = list_layers[i-1][\"ou_c\"]\n",
        "              in_f = in_c * in_w * in_h\n",
        "              #print(in_f)\n",
        "              self.model.add_module(f'linear{i+1}', nn.Linear(in_features=in_f, out_features=list_layers[i][\"ou_c\"]))\n",
        "              self.model.add_module(f'relu{i+1}', nn.ReLU(inplace=True))\n",
        "              self.model.add_module(f'batchnorm{i+1}', nn.BatchNorm1d(num_features=list_layers[i][\"ou_c\"]))\n",
        "\n",
        "          else :\n",
        "            #self.model.add_module(f'dropout{i+1}', nn.Dropout(dropout_rate))\n",
        "\n",
        "            if i == len(list_layers) - 1:\n",
        "                self.model.add_module(f'linear{i+1}', nn.Linear(list_layers[i-1][\"ou_c\"], output_dim))\n",
        "                #self.model.add_module(f'batchnorm{i+1}', nn.BatchNorm1d(num_features=output_dim))\n",
        "                #self.model.add_module('softmax', nn.Softmax(dim=1))\n",
        "            else:\n",
        "                self.model.add_module(f'linear{i+1}', nn.Linear(list_layers[i-1][\"ou_c\"], list_layers[i][\"ou_c\"]))\n",
        "                self.model.add_module(f'relu{i+1}', nn.ReLU(inplace=True))\n",
        "                self.model.add_module(f'batchnorm{i+1}', nn.BatchNorm1d(num_features=list_layers[i][\"ou_c\"]))"
      ],
      "metadata": {
        "id": "ak1-GtviUgmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Particle():\n",
        "    def __init__(self, min_layer, max_layer, max_pool_layers, input_width, input_height, input_channels, \\\n",
        "        conv_prob, pool_prob, fc_prob, max_conv_kernel, max_out_ch, max_fc_neurons, output_dim):\n",
        "        self.input_width = input_width\n",
        "        self.input_height = input_height\n",
        "        self.input_channels = input_channels\n",
        "\n",
        "        self.num_pool_layers = 0\n",
        "        self.max_pool_layers = max_pool_layers\n",
        "\n",
        "        self.feature_width = input_width\n",
        "        self.feature_height = input_height\n",
        "\n",
        "        self.depth = np.random.randint(min_layer, max_layer)\n",
        "        self.conv_prob = conv_prob\n",
        "        self.pool_prob = pool_prob\n",
        "        self.fc_prob = fc_prob\n",
        "        self.max_conv_kernel = max_conv_kernel\n",
        "        self.max_out_ch = max_out_ch\n",
        "\n",
        "        self.max_fc_neurons = max_fc_neurons\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.layers = []\n",
        "        self.acc = None\n",
        "        self.vel = [] # Initial velocity\n",
        "        self.pBest = []\n",
        "\n",
        "        # Build particle architecture\n",
        "        self.initialization()\n",
        "\n",
        "        # Update initial velocity ??\n",
        "        for i in range(len(self.layers)):\n",
        "            if self.layers[i][\"type\"] != \"fc\":\n",
        "                self.vel.append({\"type\": \"keep\"})\n",
        "            else:\n",
        "                self.vel.append({\"type\": \"keep_fc\"})\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "        self.pBest = deepcopy(self)\n",
        "\n",
        "        self.criterion = None\n",
        "\n",
        "        self.optimizer = None\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          self.device_name = torch.device(\"cuda\")\n",
        "        else:\n",
        "          self.device_name = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        string = \"\"\n",
        "        for z in range(len(self.layers)):\n",
        "            string = string + self.layers[z][\"type\"] + \" | \"\n",
        "\n",
        "        return string\n",
        "\n",
        "    def initialization(self):\n",
        "        out_channel = np.random.randint(3, self.max_out_ch)\n",
        "        conv_kernel = np.random.randint(3, self.max_conv_kernel)\n",
        "\n",
        "        # First layer is always a convolution layer\n",
        "        self.layers.append({\"type\": \"conv\", \"ou_c\": out_channel, \"kernel\": conv_kernel})\n",
        "\n",
        "        conv_prob = self.conv_prob\n",
        "        pool_prob = self.pool_prob\n",
        "        fc_prob = self.fc_prob\n",
        "\n",
        "        for i in range(1, self.depth):\n",
        "            if self.layers[-1][\"type\"] == \"fc\":\n",
        "                layer_type = 1.1\n",
        "            else:\n",
        "                layer_type = np.random.rand()\n",
        "\n",
        "            if layer_type < conv_prob:\n",
        "                self.layers = utils1.add_conv(self.layers, self.max_out_ch, self.max_conv_kernel)\n",
        "\n",
        "            elif layer_type >= conv_prob and layer_type <= pool_prob:\n",
        "                self.layers, self.num_pool_layers = utils1.add_pool(self.layers, self.fc_prob, self.num_pool_layers,\n",
        "                                                                   self.max_pool_layers, self.max_out_ch,\n",
        "                                                                   self.max_conv_kernel, self.max_fc_neurons,\n",
        "                                                                   self.output_dim)\n",
        "\n",
        "            elif layer_type >= fc_prob:\n",
        "                self.layers = utils1.add_fc(self.layers, self.max_fc_neurons)\n",
        "\n",
        "        self.layers[-1] = {\"type\": \"fc\", \"ou_c\": self.output_dim, \"kernel\": -1}\n",
        "\n",
        "\n",
        "    def velocity(self, gBest, Cg):\n",
        "        self.vel = utils1.computeVelocity(gBest, self.pBest.layers, self.layers, Cg)\n",
        "\n",
        "    def update(self):\n",
        "        new_p = utils1.updateParticle(self.layers, self.vel)\n",
        "        new_p = self.validate(new_p)\n",
        "\n",
        "        self.layers = new_p\n",
        "        self.model = None\n",
        "\n",
        "    def validate(self, list_layers):\n",
        "        # Last layer should always be a fc with number of neurons equal to the number of outputs\n",
        "        list_layers[-1] = {\"type\": \"fc\", \"ou_c\": self.output_dim, \"kernel\": -1}\n",
        "\n",
        "        # Remove excess of Pooling layers\n",
        "        self.num_pool_layers = 0\n",
        "        for i in range(len(list_layers)):\n",
        "            if list_layers[i][\"type\"] == \"max_pool\" or list_layers[i][\"type\"] == \"avg_pool\":\n",
        "                self.num_pool_layers += 1\n",
        "\n",
        "                if self.num_pool_layers >= self.max_pool_layers:\n",
        "                    list_layers[i][\"type\"] = \"remove\"\n",
        "\n",
        "\n",
        "        # Now, fix the inputs of each conv and pool layers\n",
        "        updated_list_layers = []\n",
        "\n",
        "        for i in range(0, len(list_layers)):\n",
        "            if list_layers[i][\"type\"] != \"remove\":\n",
        "                if list_layers[i][\"type\"] == \"conv\":\n",
        "                    updated_list_layers.append({\"type\": \"conv\", \"ou_c\": list_layers[i][\"ou_c\"],\n",
        "                                                \"kernel\": list_layers[i][\"kernel\"]})\n",
        "\n",
        "                if list_layers[i][\"type\"] == \"fc\":\n",
        "                    updated_list_layers.append(list_layers[i])\n",
        "\n",
        "                if list_layers[i][\"type\"] == \"max_pool\":\n",
        "                    updated_list_layers.append({\"type\": \"max_pool\", \"ou_c\": list_layers[i-1][\"ou_c\"], \"kernel\": 2})\n",
        "\n",
        "                if list_layers[i][\"type\"] == \"avg_pool\":\n",
        "                    updated_list_layers.append({\"type\": \"avg_pool\", \"ou_c\": list_layers[i-1][\"ou_c\"], \"kernel\": 2})\n",
        "\n",
        "        return updated_list_layers\n",
        "\n",
        "    ##### Model methods ####\n",
        "    def model_compile(self, dropout_rate):\n",
        "      self.model = p_model( list_layers=self.layers, input_width=self.input_width, input_height=self.input_height,\n",
        "                           input_channels=self.input_channels, output_dim=self.output_dim, dropout_rate=dropout_rate)\n",
        "\n",
        "      self.layers = self.model.list_layers\n",
        "\n",
        "      learning_rate = 0.001\n",
        "      self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "      self.criterion = nn.CrossEntropyLoss()\n",
        "      self.model.to(self.device_name)\n",
        "\n",
        "\n",
        "    def model_summary(self):\n",
        "      print(self.model)\n",
        "      summary(self.model, (self.input_channels, self.input_height, self.input_width), 16, self.device_name.type)\n",
        "\n",
        "\n",
        "    def model_fit(self, dataloader, batch_size, epochs):\n",
        "      train_loss_list = []\n",
        "      train_acc_list = []\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          self.model.train()\n",
        "          running_loss = 0.0\n",
        "          correct = 0\n",
        "          total = 0\n",
        "\n",
        "          for i, data in enumerate(dataloader, 0):\n",
        "              inputs, labels = data\n",
        "              inputs, labels = inputs.to(self.device_name), labels.to(self.device_name)\n",
        "              self.optimizer.zero_grad()\n",
        "\n",
        "              outputs = self.model(inputs)\n",
        "              loss = self.criterion(outputs, labels)\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "\n",
        "              running_loss += loss.item()\n",
        "\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "          epoch_loss = running_loss / len(dataloader)\n",
        "          epoch_acc = correct / total\n",
        "\n",
        "          train_loss_list.append(epoch_loss)\n",
        "          train_acc_list.append(epoch_acc)\n",
        "\n",
        "      return {'loss': train_loss_list, 'accuracy': train_acc_list}\n",
        "\n",
        "    def evaluate(self, dataloader, batch_size):\n",
        "      self.model.eval()\n",
        "      test_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in dataloader:\n",
        "              inputs, labels = inputs.to(self.device_name), labels.to(self.device_name)\n",
        "              outputs = self.model(inputs)\n",
        "              loss = self.criterion(outputs, labels)\n",
        "              test_loss += loss.item()\n",
        "\n",
        "              _, predicted = torch.max(outputs, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "      accuracy = correct / total\n",
        "      average_loss = test_loss / len(dataloader)\n",
        "\n",
        "      return {'loss': average_loss, 'accuracy': accuracy}\n",
        "\n",
        "    def model_fit_complete(self, dataloader1, dataloader2, batch_size, epochs):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "          #training\n",
        "          self.model.train()\n",
        "          running_loss = 0.0\n",
        "          correct = 0\n",
        "          total = 0\n",
        "\n",
        "          for i, data in enumerate(dataloader1, 0):\n",
        "              inputs, labels = data\n",
        "              inputs, labels = inputs.to(self.device_name), labels.to(self.device_name)\n",
        "              self.optimizer.zero_grad()\n",
        "\n",
        "              outputs = self.model(inputs)\n",
        "              loss = self.criterion(outputs, labels)\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "\n",
        "              running_loss += loss.item()\n",
        "\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "          epoch_loss = running_loss / len(dataloader1)\n",
        "          epoch_acc = correct / total\n",
        "\n",
        "          train_losses.append(epoch_loss)\n",
        "          train_accuracies.append(epoch_acc)\n",
        "\n",
        "          # Validation\n",
        "          self.model.eval()\n",
        "          test_loss = 0.0\n",
        "          correct = 0\n",
        "          total = 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for inputs, labels in dataloader2:\n",
        "                  inputs, labels = inputs.to(self.device_name), labels.to(self.device_name)\n",
        "                  outputs = self.model(inputs)\n",
        "                  loss = self.criterion(outputs, labels)\n",
        "                  test_loss += loss.item()\n",
        "\n",
        "                  _, predicted = torch.max(outputs, 1)\n",
        "                  total += labels.size(0)\n",
        "                  correct += (predicted == labels).sum().item()\n",
        "\n",
        "          accuracy = correct / total\n",
        "          average_loss = test_loss / len(dataloader2)\n",
        "\n",
        "          val_losses.append(average_loss)\n",
        "          val_accuracies.append(accuracy)\n",
        "\n",
        "          print(f'Epoch [{epoch+1}/{epochs}], '\n",
        "          f'Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.2%}, '\n",
        "          f'Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.2%}')\n",
        "\n",
        "        return {'train loss': train_losses, 'train accuracy': train_accuracies, 'val loss': val_losses, 'val accuracy': val_accuracies}\n",
        "\n",
        "\n",
        "    def model_delete(self):\n",
        "      del self.model\n",
        "      torch.cuda.empty_cache()\n",
        "      self.model = None"
      ],
      "metadata": {
        "id": "hgcnDNUUa0VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test = Particle(min_layer=3, max_layer=10, max_pool_layers=4, input_width=28, input_height=28, input_channels=1, \\\n",
        "#                conv_prob=0.2, pool_prob=0.3, fc_prob=0.4, max_conv_kernel=4, max_out_ch=16, max_fc_neurons=3*28*28, output_dim=4)\n",
        "#test.layers"
      ],
      "metadata": {
        "id": "Z9oWblh2bXJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test.model_compile(dropout_rate=0.5)"
      ],
      "metadata": {
        "id": "O2JmsgAAg2pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test.model"
      ],
      "metadata": {
        "id": "4bT0IVMlhfMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    return sum(params)"
      ],
      "metadata": {
        "id": "nPCrGFkdixbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count_parameters(test.model)"
      ],
      "metadata": {
        "id": "etgb4x5rjZc2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}